import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import scipy.io as sio
import keras
from google.colab import drive
from keras import backend as K
import seaborn as sns
from scipy.stats import gaussian_kde

# Montar o Google Drive
drive.mount('/content/drive')

# Caminho para os diretórios com os arquivos de Massa 0, Massa 1 e Massa 2
pasta0 = '/content/drive/MyDrive/IC/porticoamarelo/caso7'
pasta1 = '/content/drive/MyDrive/IC/porticoamarelo/caso9'
pasta2 = '/content/drive/MyDrive/IC/porticoamarelo/caso8'

def carregarEDividir(directory, segment_size=2000):
    data = []
    files = os.listdir(directory)
    files.sort()

    for arquivo in files:
        if arquivo.endswith(".mat"):
            caminho_arquivo = os.path.join(directory, arquivo)
            arquivo_mat = sio.loadmat(caminho_arquivo)
            dados = arquivo_mat['X'][:, 3]

            # Calcular o número de segmentos com base no tamanho desejado
            num_segments = len(dados) // segment_size

            # Garantir que o número de segmentos seja um número inteiro
            num_segments = int(num_segments)

            # Dividir o sinal
            segmented_data = dados[:num_segments * segment_size].reshape(num_segments, segment_size)

            # Adicionar os segmentos ao array principal
            data.append(segmented_data)

    return np.array(data)



# Carregar os dados de Massa 0
massa0 = carregarEDividir(pasta0)

# Carregar os dados de Massa 1
massa1 = carregarEDividir(pasta1)

# Carregar os dados de Massa 2
massa2 = carregarEDividir(pasta2)



massa0 = massa0.reshape(-1, massa0.shape[2])
massa1 = massa1.reshape(-1, massa1.shape[2])
massa2 = massa2.reshape(-1, massa2.shape[2])



def calcular_fft(dados, freq_limite=15, freq_minima=2):
    fft_results = []
    D = len(dados)
    frequencia_amostragem = 200
    frequencies = np.fft.fftfreq(dados.shape[1], 1 / frequencia_amostragem)[:dados.shape[1] // 2]

    # Encontre o índice correspondente à frequência mínima desejada
    idx_freq_minima = int(freq_minima * 10)

    for signal in dados:
        fft_result = np.fft.fft(signal)
        fft_result = np.abs(fft_result[:len(fft_result) // 2]) / D

        # Aplique o limite superior de frequência
        fft_result = fft_result[:freq_limite * 10]

        # Aplique o limite inferior de frequência
        fft_result = fft_result[idx_freq_minima:]

        fft_results.append(fft_result)

    # Atualize as frequências de acordo com os limites aplicados
    frequencies = frequencies[idx_freq_minima:freq_limite * 10]

    return frequencies, np.array(fft_results)


# Aplicar a FFT para cada conjunto de sinais em cada massa

frequencies_massa0, massa0 = calcular_fft(massa0)
frequencies_massa1, massa1 = calcular_fft(massa1)
frequencies_massa2, massa2 = calcular_fft(massa2)


# Calcular a média e o desvio padrão dos dados
media_massa0 = np.mean(massa0)
desvio_massa0 = np.std(massa0)

media_massa1 = np.mean(massa1)
desvio_massa1 = np.std(massa1)

media_massa2 = np.mean(massa2)
desvio_massa2 = np.std(massa2)

# Normalizar
massa0 = (massa0 - media_massa0) / desvio_massa0
massa1 = (massa1 - media_massa1) / desvio_massa1
massa2 = (massa2 - media_massa2) / desvio_massa2

# Expanda as dimensões dos dados de entrada
massa0 = np.expand_dims(massa0, axis=-1)
massa1 = np.expand_dims(massa1, axis=-1)
massa2 = np.expand_dims(massa2, axis=-1)

learning_rate= 0.007114078294468303
epochs= 75
batch_size= 13


import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# Definição dos parâmetros do CAE
input_shape = massa0.shape[1:]
latent_dim = 12  # Dimensão do espaço latente



# Número de repetições desejadas
num_repeticoes = 10

# Lista para armazenar os erros residuais de cada repetição
erros_residuais_por_repeticao = []

# Lista para armazenar os erros residuais totais
erros_residuais_totais = []

resi_latente_train_acumulado = []
resi_latente_teste_acumulado = []
resi_latente_massa1_acumulado = []
resi_latente_massa2_acumulado = []


for repeticao in range(num_repeticoes):
    print(f'Repetição {repeticao + 1}')

    # Dividir massa0 em massa0Treino e massa0Teste
    np.random.shuffle(massa0)
    massa0Treino = massa0[:60]
    massa0Teste = massa0[60:]

    # Definição do modelo CAE
    inputs = keras.Input(shape=input_shape)
    x = keras.layers.Conv1D(16, 4, activation='relu', padding='same')(inputs)
    x = keras.layers.MaxPooling1D(2, padding='same')(x)
    x = keras.layers.Conv1D(32, 4, activation='relu', padding='same')(x)
    encoded = keras.layers.MaxPooling1D(2, padding='same')(x)

    # Criação da representação latente e decodificação
    x = keras.layers.Conv1D(32, 2, activation='relu', padding='same')(encoded)
    x = keras.layers.UpSampling1D(2)(x)
    x = keras.layers.Conv1D(16, 2, activation='linear', padding='valid')(x)
    x = keras.layers.UpSampling1D(2)(x)
    decoded = keras.layers.Conv1D(1, 2, activation='linear', padding='same')(x)

    cae = keras.Model(inputs, decoded, name='conv_autoencoder')

    # Compilando o modelo
    cae.compile(optimizer='adam', loss='mean_absolute_error')


    # Treinamento do AE com os dados de Massa 0
    cae.fit(massa0Treino, massa0Treino, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)

    # Reconstruir os sinais
    massa0_reconstruidoTreino = cae.predict(massa0Treino)
    massa0_reconstruidoTeste = cae.predict(massa0Teste)
    massa1_reconstruido = cae.predict(massa1)
    massa2_reconstruido = cae.predict(massa2)
    encoder = keras.Model(inputs, encoded)


    # Coletar resíduos da camada latente para Massa 0 Treino
    resi_latente_train = cae.predict(massa0Treino)

    # Coletar resíduos da camada latente para Massa 0 Teste
    resi_latente_teste = cae.predict(massa0Teste)

    # Coletar resíduos da camada latente para Massa 1
    resi_latente_massa1 = cae.predict(massa1)

    # Coletar resíduos da camada latente para Massa 2
    resi_latente_massa2 = cae.predict(massa2)


    # Acumular os resíduos da camada latente para Massa 0 Treino
    resi_latente_train_acumulado.append(resi_latente_train)

    # Acumular os resíduos da camada latente para Massa 0 Teste
    resi_latente_teste_acumulado.append(resi_latente_teste)

    # Acumular os resíduos da camada latente para Massa 1
    resi_latente_massa1_acumulado.append(resi_latente_massa1)

    # Acumular os resíduos da camada latente para Massa 2
    resi_latente_massa2_acumulado.append(resi_latente_massa2)


# Calcular a média dos erros residuais para cada ponto
media_erros_residuais = np.mean(erros_residuais_por_repeticao, axis=0)



# Converter as listas acumuladas em matrizes NumPy
resi_latente_train_acumulado = np.array(resi_latente_train_acumulado)
resi_latente_teste_acumulado = np.array(resi_latente_teste_acumulado)
resi_latente_massa1_acumulado = np.array(resi_latente_massa1_acumulado)
resi_latente_massa2_acumulado = np.array(resi_latente_massa2_acumulado)



# Calcular a média e o desvio padrão dos resíduos da camada latente para cada classe
media_resi_latente_massa0 = np.mean(resi_latente_train_acumulado, axis=(0, 1))
desvio_resi_latente_massa0 = np.std(resi_latente_train_acumulado, axis=(0, 1))
media_resi_latente_teste_massa0 = np.mean(resi_latente_teste_acumulado, axis=(0, 1))
desvio_resi_latente_teste_massa0 = np.std(resi_latente_teste_acumulado, axis=(0, 1))
media_resi_latente_massa1 = np.mean(resi_latente_massa1_acumulado, axis=(0, 1))
desvio_resi_latente_massa1 = np.std(resi_latente_massa1_acumulado, axis=(0, 1))
media_resi_latente_massa2 = np.mean(resi_latente_massa2_acumulado, axis=(0, 1))
desvio_resi_latente_massa2 = np.std(resi_latente_massa2_acumulado, axis=(0, 1))



# Converter as listas acumuladas em matrizes NumPy
resi_latente_train_acumulado = np.array(resi_latente_train_acumulado)
resi_latente_teste_acumulado = np.array(resi_latente_teste_acumulado)
resi_latente_massa1_acumulado = np.array(resi_latente_massa1_acumulado)
resi_latente_massa2_acumulado = np.array(resi_latente_massa2_acumulado)


import numpy as np
from scipy.stats import f
import matplotlib.pyplot as plt

# Parâmetros
r = 15  # Subgroups com r observações de x (quantidade de sinais)
m = latent_dim  # Número de características da camada latente
fontsize = 29

# Matriz de características da camada latente para treinamento e teste

features_train = np.concatenate(resi_latente_train_acumulado, axis=0)
features_train = features_train[:, :, 0]  # Selecionar apenas as duas primeiras dimensões

features_test = np.concatenate(resi_latente_teste_acumulado, axis=0)
features_test = features_test[:, :, 0]  # Selecionar apenas as duas primeiras dimensões

features_massa1 = np.concatenate(resi_latente_massa1_acumulado, axis=0)
features_massa1 = features_massa1[:, :, 0]  # Selecionar apenas as duas primeiras dimensões

features_massa2 = np.concatenate(resi_latente_massa2_acumulado, axis=0)
features_massa2 = features_massa2[:, :, 0]  # Selecionar apenas as duas primeiras dimensões

# Número de grupos coletados no estado de referência (treinamento) e teste
s_train = len(resi_latente_train_acumulado)
s_test = len(resi_latente_teste_acumulado)
s_massa1 = len(resi_latente_massa1_acumulado)
s_massa2 = len(resi_latente_massa2_acumulado)

# Estimador S1:
S = np.cov(features_train, rowvar=False)  # Matriz de covariância das características da camada latente

# T-squared para a fase de treinamento
x_ref_m = np.mean(features_train, axis=0)  # Média dos dados de treinamento
T_squared_train = []

for i in range(0, len(features_train), r):
    x = features_train[i:i+r, :]  # "Novos" dados
    x_m = np.mean(x, axis=0)  # Média dos "novos" dados
    T_squared_train.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a fase de teste
T_squared_test = []

for i in range(0, len(features_test), r):
    x = features_test[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_test.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 1
T_squared_massa1 = []

for i in range(0, len(features_massa1), r):
    x = features_massa1[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa1.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 2
T_squared_massa2 = []

for i in range(0, len(features_massa2), r):
    x = features_massa2[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa2.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))


# Calcular o percentil 95 dos valores de T_squared_train
percentile_95 = np.percentile(T_squared_train, 95)

# Definir a UCL como um múltiplo do percentil 95
UCL = percentile_95

# Criar uma lista única de UCL para todas as fases
UCL_axis = np.full(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2), UCL)

# Plotar carta de controle T^2 de Hotelling para treinamento, teste, Massa 1, Massa 2, Massa 3 e Massa 4
plt.figure(figsize=(16, 8))
plt.plot(np.arange(len(T_squared_train)), T_squared_train, 'x', color='#2924be', label='training')
plt.plot(np.arange(len(T_squared_train), len(T_squared_train) + len(T_squared_test)), T_squared_test, 'h', color='#71bdd7', fillstyle='none', label='validation')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1)), T_squared_massa1, linestyle='None', marker=(8, 2, 0), markersize=5, mew=0.75, color='#9c1732', label='monitoring')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2)), T_squared_massa2, 'D', color='#82b33d', fillstyle='none', markersize=4, mew=0.75, label='monitoring')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) ), UCL_axis, 'k', linewidth=3, label='UCL')
plt.yscale('log')

plt.xlabel('data subgroups',fontsize = fontsize)
plt.ylabel('T²',fontsize = fontsize)
plt.legend(fontsize = 24)
plt.tick_params(axis='both', labelsize=fontsize-3)
plt.xlim(0, len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2))

# Adicionar linhas verticais tracejadas a cada 200 pontos
for i in range(0, 1000, 60):
    plt.axvline(x=i, linestyle='--', color='gray', linewidth=1, dashes=(6, 6))

plt.figure(figsize=(16, 8))
plt.show()
