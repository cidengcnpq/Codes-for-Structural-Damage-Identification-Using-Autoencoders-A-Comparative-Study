import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import scipy.io as sio
import keras
from google.colab import drive
from keras import backend as K
import seaborn as sns
from scipy.stats import gaussian_kde

# Montar o Google Drive
drive.mount('/content/drive')

# Caminho para os diretórios com os arquivos de Massa 0, Massa 1 e Massa 2
pasta0 = '/content/drive/MyDrive/IC/portico/Massa 0'
pasta1 = '/content/drive/MyDrive/IC/portico/Massa 1'
pasta2 = '/content/drive/MyDrive/IC/portico/Massa 2'
pasta3 = '/content/drive/MyDrive/IC/portico/Massa 3'
pasta4 = '/content/drive/MyDrive/IC/portico/Massa 4'

def calcular_fft(dados, num_pontos=2000, freq_limite=40):
    dados = dados[:num_pontos]  # Pegar apenas os primeiros 2000 pontos

    D = len(dados)
    Fs = 500  # Frequência de amostragem
    dt = 1 / Fs  # Período de amostragem
    T = D * dt  # Período total

    df = 1 / T
    freq = np.arange(0, D) * df

    X_f = np.fft.fft(dados)  # Calcula a FFT
    X_f = X_f[:D // 2]  # Descarta metade das amostras
    X_f = np.abs(X_f)

    # Encontre o índice correspondente à frequência limite
    indice_limite = int(freq_limite / df)

    freq = freq[:indice_limite]  # Mantém apenas as frequências até freq_limite
    X_f = X_f[:indice_limite]  # Mantém apenas as amplitudes correspondentes

    return freq, X_f


learning_rate= 0.019382487167867446
epochs= 5
batch_size= 3
intermediate_dim= 80
latent_dim= 40


# Carregar os dados de Massa 0
massa0 = []
arquivos_massa0 = os.listdir(pasta0)
for arquivo in arquivos_massa0[:300]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(pasta0, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['X'][:, 0]
        freq, X_f = calcular_fft(dados)
        massa0.append(X_f)

massa0 = np.array(massa0)

# Carregar os dados de Massa 1
massa1 = []
arquivos_massa1 = os.listdir(pasta1)
for arquivo in arquivos_massa1[:300]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(pasta1, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['X'][:, 0]
        f, X_f = calcular_fft(dados)  # Calcula a FFT nos dados
        massa1.append(X_f)

massa1 = np.array(massa1)

# Carregar os dados de Massa 2
massa2 = []
arquivos_massa2 = os.listdir(pasta2)
for arquivo in arquivos_massa2[:300]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(pasta2, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['X'][:, 0]
        f, X_f = calcular_fft(dados)  # Calcula a FFT nos dados
        massa2.append(X_f)

massa2 = np.array(massa2)

# Carregar os dados de Massa 3
massa3 = []
arquivos_massa3 = os.listdir(pasta3)
for arquivo in arquivos_massa3[:300]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(pasta3, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['X'][:, 0]
        f, X_f = calcular_fft(dados)  # Calcula a FFT nos dados
        massa3.append(X_f)

massa3 = np.array(massa3)

# Carregar os dados de Massa 4
massa4 = []
arquivos_massa4 = os.listdir(pasta4)
for arquivo in arquivos_massa4[:300]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(pasta4, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['X'][:, 0]
        f, X_f = calcular_fft(dados)  # Calcula a FFT nos dados
        massa4.append(X_f)

massa4 = np.array(massa4)



# Calcular a média e o desvio padrão dos dados
media_massa0 = np.mean(massa0)
desvio_massa0 = np.std(massa0)

media_massa1 = np.mean(massa1)
desvio_massa1 = np.std(massa1)

media_massa2 = np.mean(massa2)
desvio_massa2 = np.std(massa2)

media_massa3 = np.mean(massa3)
desvio_massa3 = np.std(massa3)

media_massa4 = np.mean(massa4)
desvio_massa4 = np.std(massa4)

# Normalizar
massa0 = (massa0 - media_massa0) / desvio_massa0
massa1 = (massa1 - media_massa1) / desvio_massa1
massa2 = (massa2 - media_massa2) / desvio_massa2
massa3 = (massa3 - media_massa3) / desvio_massa3
massa4 = (massa4 - media_massa4) / desvio_massa4



# Definir e treinar o VAE
original_dim = massa0.shape[1]
inputs = keras.Input(shape=(original_dim,))
h = keras.layers.Dense(intermediate_dim, activation='relu')(inputs)
z_mean = keras.layers.Dense(latent_dim)(h)
z_log_sigma = keras.layers.Dense(latent_dim)(h)

def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=0.1)
    return z_mean + K.exp(0.5*z_log_sigma) * epsilon



# Número de repetições desejadas
num_repeticoes = 10

# Lista para armazenar os erros residuais de cada repetição
erros_residuais_por_repeticao = []

# Lista para armazenar os erros residuais totais
erros_residuais_totais = []

resi_latente_train_acumulado = []
resi_latente_teste_acumulado = []
resi_latente_massa1_acumulado = []
resi_latente_massa2_acumulado = []
resi_latente_massa3_acumulado = []
resi_latente_massa4_acumulado = []

# Loop para repetições
for repeticao in range(num_repeticoes):
    print(f'Repetição {repeticao + 1}')
    # Dividir massa0 em massa0Treino e massa0Teste
    np.random.shuffle(massa0)
    massa0Treino = massa0[:200]
    massa0Teste = massa0[200:]

    z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_sigma])

    encoder = tf.keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')

    latent_inputs = tf.keras.Input(shape=(latent_dim,))
    x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
    outputs = tf.keras.layers.Dense(original_dim, activation='linear')(x)
    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')

    outputs = decoder(encoder(inputs)[2])
    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')

    reconstruction_loss = tf.reduce_mean(tf.abs(inputs - outputs))
    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma))
    vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))

    # Treinamento do VAE com os dados de Massa 0
    vae.fit(massa0Treino, massa0Treino, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)

    # Reconstruir os sinais
    massa0_reconstruidoTreino = vae.predict(massa0Treino)
    massa0_reconstruidoTeste = vae.predict(massa0Teste)
    massa1_reconstruido = vae.predict(massa1)
    massa2_reconstruido = vae.predict(massa2)
    massa3_reconstruido = vae.predict(massa3)
    massa4_reconstruido = vae.predict(massa4)

    # Calcular os erros residuais e armazená-los
    erroTreino = np.mean(np.square(massa0Treino - massa0_reconstruidoTreino), axis=1)
    erro_massa0Teste = np.mean(np.square(massa0Teste - massa0_reconstruidoTeste), axis=1)
    erro_massa1 = np.mean(np.square(massa1 - massa1_reconstruido), axis=1)
    erro_massa2 = np.mean(np.square(massa2 - massa2_reconstruido), axis=1)
    erro_massa3 = np.mean(np.square(massa3 - massa3_reconstruido), axis=1)
    erro_massa4 = np.mean(np.square(massa4 - massa4_reconstruido), axis=1)

    erros_residuais = np.concatenate([erroTreino, erro_massa0Teste, erro_massa1, erro_massa2, erro_massa3, erro_massa4])
    erros_residuais_por_repeticao.append(erros_residuais)

    # Coletar resíduos da camada latente para Massa 0 Treino
    resi_latente_train = encoder.predict(massa0Treino)[2]

    # Coletar resíduos da camada latente para Massa 0 Teste
    resi_latente_teste = encoder.predict(massa0Teste)[2]

    # Coletar resíduos da camada latente para Massa 1
    resi_latente_massa1 = encoder.predict(massa1)[2]

    # Coletar resíduos da camada latente para Massa 2
    resi_latente_massa2 = encoder.predict(massa2)[2]

    # Coletar resíduos da camada latente para Massa 3
    resi_latente_massa3 = encoder.predict(massa3)[2]

    # Coletar resíduos da camada latente para Massa 4
    resi_latente_massa4 = encoder.predict(massa4)[2]

    # Acumular os resíduos da camada latente para Massa 0 Treino
    resi_latente_train_acumulado.append(encoder.predict(massa0Treino)[2])

    # Acumular os resíduos da camada latente para Massa 0 Teste
    resi_latente_teste_acumulado.append(encoder.predict(massa0Teste)[2])

    # Acumular os resíduos da camada latente para Massa 1
    resi_latente_massa1_acumulado.append(encoder.predict(massa1)[2])

    # Acumular os resíduos da camada latente para Massa 2
    resi_latente_massa2_acumulado.append(encoder.predict(massa2)[2])

    # Acumular os resíduos da camada latente para Massa 3
    resi_latente_massa3_acumulado.append(encoder.predict(massa3)[2])

    # Acumular os resíduos da camada latente para Massa 4
    resi_latente_massa4_acumulado.append(encoder.predict(massa4)[2])



# Calcular a média e o desvio padrão dos resíduos da camada latente para cada classe
media_resi_latente_massa0 = np.mean(resi_latente_train_acumulado, axis=(0, 1))
desvio_resi_latente_massa0 = np.std(resi_latente_train_acumulado, axis=(0, 1))
media_resi_latente_teste_massa0 = np.mean(resi_latente_teste_acumulado, axis=(0, 1))
desvio_resi_latente_teste_massa0 = np.std(resi_latente_teste_acumulado, axis=(0, 1))
media_resi_latente_massa1 = np.mean(resi_latente_massa1_acumulado, axis=(0, 1))
desvio_resi_latente_massa1 = np.std(resi_latente_massa1_acumulado, axis=(0, 1))
media_resi_latente_massa2 = np.mean(resi_latente_massa2_acumulado, axis=(0, 1))
desvio_resi_latente_massa2 = np.std(resi_latente_massa2_acumulado, axis=(0, 1))
media_resi_latente_massa3 = np.mean(resi_latente_massa3_acumulado, axis=(0, 1))
desvio_resi_latente_massa3 = np.std(resi_latente_massa3_acumulado, axis=(0, 1))
media_resi_latente_massa4 = np.mean(resi_latente_massa4_acumulado, axis=(0, 1))
desvio_resi_latente_massa4 = np.std(resi_latente_massa4_acumulado, axis=(0, 1))



# Converter as listas acumuladas em matrizes NumPy
resi_latente_train_acumulado = np.array(resi_latente_train_acumulado)
resi_latente_teste_acumulado = np.array(resi_latente_teste_acumulado)
resi_latente_massa1_acumulado = np.array(resi_latente_massa1_acumulado)
resi_latente_massa2_acumulado = np.array(resi_latente_massa2_acumulado)
resi_latente_massa3_acumulado = np.array(resi_latente_massa3_acumulado)
resi_latente_massa4_acumulado = np.array(resi_latente_massa4_acumulado)


import numpy as np
from scipy.stats import f
import matplotlib.pyplot as plt

# Parâmetros
r = 15  # Subgroups com r observações de x (quantidade de sinais)
m = latent_dim  # Número de características da camada latente

# Matriz de características da camada latente para treinamento e teste
features_train = np.concatenate(resi_latente_train_acumulado, axis=0)
features_test = np.concatenate(resi_latente_teste_acumulado, axis=0)
features_massa1 = np.concatenate(resi_latente_massa1_acumulado, axis=0)
features_massa2 = np.concatenate(resi_latente_massa2_acumulado, axis=0)
features_massa3 = np.concatenate(resi_latente_massa3_acumulado, axis=0)
features_massa4 = np.concatenate(resi_latente_massa4_acumulado, axis=0)

# Número de grupos coletados no estado de referência (treinamento) e teste
s_train = len(resi_latente_train_acumulado)
s_test = len(resi_latente_teste_acumulado)
s_massa1 = len(resi_latente_massa1_acumulado)
s_massa2 = len(resi_latente_massa2_acumulado)
s_massa3 = len(resi_latente_massa3_acumulado)
s_massa4 = len(resi_latente_massa4_acumulado)

# Estimador S1:
S = np.cov(features_train, rowvar=False)  # Matriz de covariância das características da camada latente

# T-squared para a fase de treinamento
x_ref_m = np.mean(features_train, axis=0)  # Média dos dados de treinamento
T_squared_train = []

for i in range(0, len(features_train), r):
    x = features_train[i:i+r, :]  # "Novos" dados
    x_m = np.mean(x, axis=0)  # Média dos "novos" dados
    T_squared_train.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a fase de teste
T_squared_test = []

for i in range(0, len(features_test), r):
    x = features_test[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_test.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 1
T_squared_massa1 = []

for i in range(0, len(features_massa1), r):
    x = features_massa1[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa1.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 2
T_squared_massa2 = []

for i in range(0, len(features_massa2), r):
    x = features_massa2[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa2.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 3
T_squared_massa3 = []

for i in range(0, len(features_massa3), r):
    x = features_massa3[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa3.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# T-squared para a Massa 4
T_squared_massa4 = []

for i in range(0, len(features_massa4), r):
    x = features_massa4[i:i+r, :]  # Novos dados
    x_m = np.mean(x, axis=0)  # Média dos novos dados
    T_squared_massa4.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

# Limite de controle superior para todas as fases (usando apenas os dados de treinamento)
F = f.ppf(0.95, m, (s_train * r) - s_train - m + 1)
UCL = ((m * (s_train - 1) * (r - 1)) / ((s_train * r) - s_train - m + 1)) * F

# Criar uma lista única de UCL para todas as fases
UCL_axis = np.full(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3) + len(T_squared_massa4), UCL)

# Plotar carta de controle T^2 de Hotelling para treinamento, teste, Massa 1, Massa 2, Massa 3 e Massa 4
plt.figure(figsize=(16, 8))
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1)), UCL_axis[len(T_squared_train) + len(T_squared_test):len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1)], 'k', linewidth=3)
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2)), UCL_axis[len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1):len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2)], 'k', linewidth=3)
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3)), UCL_axis[len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2):len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3)], 'k', linewidth=3)
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3) + len(T_squared_massa4)), UCL_axis[len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3):], 'k', linewidth=3)

plt.plot(np.arange(len(T_squared_train)), T_squared_train, 'x', color='#2924be', label='training')
plt.plot(np.arange(len(T_squared_train), len(T_squared_train) + len(T_squared_test)), T_squared_test, 'h', color='#71bdd7',fillstyle='none',label='validation')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1)), T_squared_massa1, linestyle='None', marker=(8, 2, 0), markersize=5, mew=0.75,color='#9c1732',label='monitoring')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2)), T_squared_massa2, 'D',color='#82b33d', fillstyle='none', markersize=4,mew=0.75,label='monitoring')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3)), T_squared_massa3,'s',fillstyle='none', markersize=4,mew=0.75,color='#d9561e', label='monitoring')
plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3), len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3) + len(T_squared_massa4)), T_squared_massa4, 'v',fillstyle='none', markersize=4,mew=0.75, color='#7d3d8b', label='monitoring')
plt.plot(np.arange(len(T_squared_train)), UCL_axis[:len(T_squared_train)], 'k', linewidth=3, label='UCL')
plt.plot(np.arange(len(T_squared_train), len(T_squared_train) + len(T_squared_test)), UCL_axis[len(T_squared_train):len(T_squared_train) + len(T_squared_test)], 'k', linewidth=3)

plt.yscale('log')

plt.xlabel('data subgroups')
plt.ylabel('T²')
plt.legend()
#plt.title('Carta de Controle T^2 de Hotelling - Treinamento, Teste, Massa 1, Massa 2, Massa 3 e Massa 4')
#plt.grid()
plt.xlim(0, len(T_squared_train) + len(T_squared_test) + len(T_squared_massa1) + len(T_squared_massa2) + len(T_squared_massa3) + len(T_squared_massa4))

# Adicionar linhas verticais tracejadas a cada 200 pontos
for i in range(0, 1000, 200):
    plt.axvline(x=i, linestyle='--', color='gray', linewidth=1,dashes=(6, 6))

plt.figure(figsize=(16, 8))
plt.show()
